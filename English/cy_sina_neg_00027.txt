Does software engineer use the algorithm 
0
After feeding a type of AI known as a recurrent neural network the roughly 5,000 pages of Martin 's five previous books , software engineer Zack Thoutt has used the algorithm to predict what will happen next .
algorithm	30	CT+	CT+
predict	32	ps_cue
will	34	ps_cue
########
1
<EOP> .
########
2
According to the AI 's predictions , some long_held fan theories do play out _ in the five chapters generated by the algorithm so far , Jaime ends up killing Cersei , Jon rides a dragon , and Varys poisons Daenerys .
algorithm	22	CT+	CT+
########
3
<EOP> .
########
4
https : __ github.com _ zackthoutt_got_book_6_tree_master_generated_book_v1 <EOP> .
########
5
Each chapter starts with a character 's name , just like Martin 's actual books .
like	10	ps_cue
########
6
<EOP> .
########
7
But in addition to backing up what many of us already suspect will happen , the AI also introduces some fairly unexpected plot turns that we 're pretty sure are n't going to be mirrored in either the TV show or Martin 's books , so we would n't get too excited just yet .
suspect	11	ps_cue
will	12	ps_cue
unexpected	21	ps_cue
n't	30	neg_cue
either	36	ps_cue
or	40	ps_cue
would	47	ps_cue
n't	48	neg_cue
yet	53	neg_cue
########
8
<EOP> .
########
9
For example , in the algorithm 's first chapter , written from Tyrion 's perspective , Sansa turns out to be a Baratheon .
algorithm	5	CT+	CT+
########
10
<EOP> .
########
11
There 's also the introduction of a strange , pirate_like new character called Greenbeard .
########
12
<EOP> .
########
13
`` It 's obviously not perfect , '' Thoutt told Sam Hill over at Motherboard .
not	4	neg_cue
########
14
`` It is n't building a long_term story and the grammar is n't perfect .
n't	3	neg_cue
n't	12	neg_cue
########
15
But the network is able to learn the basics of the English language and structure of George R.R. Martin 's style on its own . ''
########
16
<EOP> .
########
17
Neural networks are a type of machine learning algorithm that are inspired by the human brain 's ability to not just memorize and follow instructions , but actually learn from past experiences .
algorithm	8	CT+	CT+
not	19	neg_cue
########
18
<EOP> .
########
19
A recurrent neural network is a specific subclass , which works best when it comes to processing long sequences of data , such as lengthy text from five previous books .
########
20
<EOP> .
########
21
In theory , Thoutt 's algorithm should be able to create a true sequel to Martin 's existing work , based off things that have already happened in the novels .
algorithm	5	CT+	CT+
should	6	ps_cue
########
22
<EOP> .
########
23
But in practice , the writing is clumsy and , most of the time , nonsensical .
########
24
And it also references characters that have already died .
########
25
<EOP> .
########
26
Still , some of the lines sound fairly prophetic : <EOP> .
########
27
`` Arya saw Jon holding spears .
########
28
Your grace , '' he said to an urgent maid , afraid .
########
29
`` The crow 's eye would join you .
would	5	ps_cue
########
30
<EOP> .
########
31
`` A perfect model would take everything that has happened in the books into account and not write about characters being alive when they died two books ago , '' Thoutt told Motherboard .
would	4	ps_cue
not	16	neg_cue
########
32
<EOP> .
########
33
`` The reality , though , is that the model is n't good enough to do that .
n't	11	neg_cue
########
34
If the model were that good authors might be in trouble ... but it makes a lot of mistakes because the technology to train a perfect text generator that can remember complex plots over millions of words does n't exist yet . ''
might	7	ps_cue
can	29	ps_cue
n't	38	neg_cue
yet	40	neg_cue
########
35
<EOP> .
########
36
One of the main limitations here is the fact that the books just do n't contain enough data for an algorithm .
algorithm	20	CT+	CT+
n't	14	neg_cue
########
37
<EOP> .
########
38
Although anyone who 's read them will testify that they 're pretty damn long , they actually represent quite a small data set for a neural network to learn from .
will	6	ps_cue
########
39
<EOP> .
########
40
But at the same time they contain a whole lot of unique words , nouns , and adjectives which are n't reused , which makes it very hard for the neural network to learn patterns .
n't	20	neg_cue
########
41
| <EOP> .
########
42
Thoutt told Hill that a better source would be a book 100 times longer , but with the level of vocabulary of a children 's book .
would	7	ps_cue
########
43
<EOP> .
########
